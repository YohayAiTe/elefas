// +build !noasm
// Generated by PeachPy 0.2.0 from dense_amd64.py


// func DenseApplyF32(input_ptr *float32, kernel_ptr *float32, bias_ptr *float32, output_ptr *float32, batch_count int64, input_units int64, output_units int64)
TEXT Â·DenseApplyF32(SB),4,$0-56
	MOVQ input_ptr+0(FP), AX
	MOVQ kernel_ptr+8(FP), BX
	MOVQ bias_ptr+16(FP), CX
	MOVQ output_ptr+24(FP), DX
	MOVQ batch_count+32(FP), DI
	MOVQ input_units+40(FP), SI
	MOVQ output_units+48(FP), BP
batch_loop_begin:
		MOVQ $0, R8
		MOVQ BX, R9
		MOVQ CX, R10
output_loop_begin:
			MOVQ AX, R11
			MOVQ $0, R12
			BYTE $0xC5; BYTE $0xFC; BYTE $0x57; BYTE $0xC0 // VXORPS ymm0, ymm0, ymm0
			BYTE $0xC5; BYTE $0xF4; BYTE $0x57; BYTE $0xC9 // VXORPS ymm1, ymm1, ymm1
			BYTE $0xC5; BYTE $0xEC; BYTE $0x57; BYTE $0xD2 // VXORPS ymm2, ymm2, ymm2
			BYTE $0xC5; BYTE $0xE4; BYTE $0x57; BYTE $0xDB // VXORPS ymm3, ymm3, ymm3
			ADDQ $32, R12
			CMPQ R12, SI
			JHI vector_input_loop_end
vector_input_loop_begin:
				BYTE $0xC4; BYTE $0xC1; BYTE $0x7C; BYTE $0x10; BYTE $0x23 // VMOVUPS ymm4, [r11]
				BYTE $0xC4; BYTE $0xC1; BYTE $0x7C; BYTE $0x10; BYTE $0x6B; BYTE $0x20 // VMOVUPS ymm5, [r11 + 32]
				BYTE $0xC4; BYTE $0xC1; BYTE $0x7C; BYTE $0x10; BYTE $0x73; BYTE $0x40 // VMOVUPS ymm6, [r11 + 64]
				BYTE $0xC4; BYTE $0xC1; BYTE $0x7C; BYTE $0x10; BYTE $0x7B; BYTE $0x60 // VMOVUPS ymm7, [r11 + 96]
				BYTE $0xC4; BYTE $0xC2; BYTE $0x5D; BYTE $0xB8; BYTE $0x01 // VFMADD231PS ymm0, ymm4, [r9]
				BYTE $0xC4; BYTE $0xC2; BYTE $0x55; BYTE $0xB8; BYTE $0x49; BYTE $0x20 // VFMADD231PS ymm1, ymm5, [r9 + 32]
				BYTE $0xC4; BYTE $0xC2; BYTE $0x4D; BYTE $0xB8; BYTE $0x51; BYTE $0x40 // VFMADD231PS ymm2, ymm6, [r9 + 64]
				BYTE $0xC4; BYTE $0xC2; BYTE $0x45; BYTE $0xB8; BYTE $0x59; BYTE $0x60 // VFMADD231PS ymm3, ymm7, [r9 + 96]
				ADDQ $128, R11
				ADDQ $128, R9
				ADDQ $32, R12
				CMPQ R12, SI
				JLS vector_input_loop_begin
vector_input_loop_end:
			BYTE $0xC5; BYTE $0xFC; BYTE $0x58; BYTE $0xC1 // VADDPS ymm0, ymm0, ymm1
			BYTE $0xC5; BYTE $0xEC; BYTE $0x58; BYTE $0xD3 // VADDPS ymm2, ymm2, ymm3
			BYTE $0xC5; BYTE $0xFC; BYTE $0x58; BYTE $0xC2 // VADDPS ymm0, ymm0, ymm2
			BYTE $0xC4; BYTE $0xE3; BYTE $0x7D; BYTE $0x19; BYTE $0xC1; BYTE $0x01 // VEXTRACTF128 xmm1, ymm0, 1
			BYTE $0xC5; BYTE $0xF8; BYTE $0x58; BYTE $0xC1 // VADDPS xmm0, xmm0, xmm1
			BYTE $0xC5; BYTE $0xFB; BYTE $0x7C; BYTE $0xC0 // VHADDPS xmm0, xmm0, xmm0
			BYTE $0xC5; BYTE $0xFB; BYTE $0x7C; BYTE $0xC0 // VHADDPS xmm0, xmm0, xmm0
			BYTE $0xC4; BYTE $0xC1; BYTE $0x7A; BYTE $0x58; BYTE $0x02 // VADDSS xmm0, xmm0, [r10]
			SUBQ $32, R12
			CMPQ SI, R12
			JEQ scalar_input_loop_end
scalar_input_loop_begin:
				BYTE $0xC4; BYTE $0xC1; BYTE $0x7A; BYTE $0x10; BYTE $0x0B // VMOVSS xmm1, [r11]
				BYTE $0xC4; BYTE $0xC2; BYTE $0x71; BYTE $0xB9; BYTE $0x01 // VFMADD231SS xmm0, xmm1, [r9]
				ADDQ $4, R9
				ADDQ $4, R11
				INCQ R12
				CMPQ SI, R12
				JNE scalar_input_loop_begin
scalar_input_loop_end:
			BYTE $0xC5; BYTE $0xFA; BYTE $0x11; BYTE $0x02 // VMOVSS [rdx], xmm0
			ADDQ $4, R10
			ADDQ $4, DX
			INCQ R8
			CMPQ BP, R8
			JNE output_loop_begin
		ADDQ SI, AX
		ADDQ SI, AX
		ADDQ SI, AX
		ADDQ SI, AX
		DECQ DI
		JNE batch_loop_begin
	BYTE $0xC5; BYTE $0xF8; BYTE $0x77 // VZEROUPPER
	RET
